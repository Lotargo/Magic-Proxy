# Техническая Документация: Модуль cache_manager.py

## 1. Краткое Резюме

`cache_manager.py` — это утилитарный модуль, который реализует конфигурируемый, детерминированный кэш на базе Redis. Его основная цель — сокращение затрат и повышение производительности путем сохранения и повторного использования результатов дорогих API-вызовов. Модуль предоставляет функции для генерации кэш-ключа на основе содержимого запроса, а также для записи и чтения данных из кэша.

## 2. Архитектурная Роль и Решаемая Проблема

В системах, интенсивно использующих внешние AI API, многие запросы могут быть повторяющимися (например, запросы на перевод стандартных фраз, одинаковые запросы к RAG-системе и т.д.). Каждый такой вызов стоит денег и времени, а также расходует лимиты API (rate limits).

`cache_manager.py` решает эту проблему, внедряя слой кэширования, который перехватывает запросы. Если идентичный запрос уже выполнялся недавно, его результат мгновенно возвращается из Redis, минуя дорогостоящий и медленный вызов внешнего провайдера.

Ключевые решаемые проблемы:
* **Избыточные затраты:** Предотвращение повторных платежей за одни и те же вычисления.
* **Низкая производительность:** Мгновенный возврат кэшированных ответов вместо ожидания ответа от внешнего API.
* **Истощение лимитов:** Сокращение общего количества запросов к провайдерам, что помогает оставаться в рамках установленных rate limits.

## 3. Ключевые Функции и Их Роль

### 3.1. create_cache_key(...)

Это центральная функция модуля, отвечающая за создание уникального, но детерминированного идентификатора для кэширования.

```python
def create_cache_key(
    request_data: BaseModel,
    model_config: Dict[str, Any],
    cache_config: Dict[str, Any]
) -> Optional[str]:
    ...
```

**Алгоритм работы:**
1. Проверяет, включено ли кэширование в `cache_config` в файле `proxy_config.yaml`.
2. Находит в `cache_config.rules` правило, которое соответствует текущей модели (`internal_model_name`).
3. Если подходящее правило найдено, функция создает словарь, включая в него только те поля из тела запроса (`request_data`), которые перечислены в `rule.include_in_key`.
4. В этот словарь всегда добавляется `internal_model_name`, чтобы избежать коллизий между разными моделями при одинаковых запросах.
5. Полученный словарь сериализуется в стабильную JSON-строку (с сортировкой ключей).
6. От этой строки вычисляется хэш SHA-256, который и становится уникальным идентификатором запроса.
7. Если правило не найдено или в хэш не попало ни одно поле из запроса, функция возвращает `None`, и кэширование для данного запроса не выполняется.

**Ключевое понимание:** Ключ является детерминированным. Два запроса с абсолютно одинаковым содержимым в полях, указанных в `include_in_key`, всегда сгенерируют один и тот же ключ. Функция не использует временные метки или случайные данные.

### 3.2. set_to_cache(...) и get_from_cache(...)

Это простые асинхронные обертки для взаимодействия с Redis.

```python
async def set_to_cache(key: str, value: str, redis_client: redis.Redis, ttl_seconds: int):
    # Устанавливает значение по ключу с заданным временем жизни (TTL).
    ...

async def get_from_cache(key: str, redis_client: redis.Redis) -> Optional[str]:
    # Получает значение по ключу. Возвращает None, если ключ не найден.
    ...
```

Обе функции проверяют наличие активного клиента `redis_client` и не выполняют никаких действий, если он отсутствует, что делает их безопасными для использования даже при недоступности Redis.

## 4. Конфигурация

Поведение кэша полностью управляется через секцию `cache_settings` в файле `proxy_config.yaml`.

**Пример конфигурации:**

```yaml
cache_settings:
  # Глобальный переключатель для всей системы кэширования
  enabled: true

  # Префикс, добавляемый ко всем ключам в Redis
  key_prefix: "magic_proxy:cache:"

  # Список правил кэширования
  rules:
    # Правило №1: кэшировать запросы к моделям для эмбеддингов
    - model_names:
        - "openai_embedding_3_large"
        - "google_embedding_004"
      # Включать в ключ только поле 'input'
      include_in_key:
        - "input"
      # Хранить результат в кэше 1 час (3600 секунд)
      ttl_seconds: 3600

    # Правило №2: кэшировать запросы к определенной чат-модели
    - model_names:
        - "openai_gpt4_main"
      # Включать в ключ сообщения и параметры генерации
      include_in_key:
        - "messages"
        - "temperature"
        - "max_tokens"
      # Хранить результат 10 минут
      ttl_seconds: 600
```

## 5. Типичный Рабочий Процесс (Workflow)

Модуль `cache_manager` интегрируется в основной поток обработки запросов (например, в `main.py`) следующим образом:

**Перед выполнением запроса:**
1. Вызывается `create_cache_key()` с данными запроса и конфигурацией.
2. Если функция возвращает `key`, то...
3. Вызывается `get_from_cache(key)`.
4. Если результат (`cached_value`) найден, он немедленно возвращается клиенту, и вся дальнейшая обработка прекращается (Cache Hit).

**Если кэш пуст (Cache Miss):**
1. Запрос выполняется обычным образом через `route_request` и `execute_request_with_key_rotation`.
2. После получения успешного ответа от AI-провайдера...
3. Вызывается `set_to_cache(key, result, ttl_seconds)`, чтобы сохранить результат для будущих запросов.
4. Результат возвращается клиенту.

## 6. Архитектурная Ценность и Бизнес-Эффект

* **Сокращение операционных расходов:** Прямая экономия средств за счет уменьшения количества платных API-вызовов. Наибольший эффект достигается на часто повторяющихся запросах (эмбеддинги, стандартные чат-запросы).
* **Улучшение пользовательского опыта:** Значительное ускорение ответа для кэшированных запросов (миллисекунды вместо секунд).
* **Повышение надежности:** Снижение нагрузки на внешние API уменьшает вероятность столкновения с `RateLimitError` и временными сбоями на стороне провайдера.
* **Гибкость:** Конфигурация через YAML позволяет тонко настраивать, какие модели и какие параметры запроса кэшировать, без изменения кода.