# Technical Documentation: models.py Module

## 1. Executive Summary

`models.py` is a central module that defines data schemas for the entire application using the Pydantic library. It functions as a strictly typed "API contract," ensuring the validation, self-documentation, and reliability of all data circulating within the system. The module's primary role is to describe structures compatible with the OpenAI API standard, making the gateway transparent and easily integrable for existing clients.

## 2. Architectural Role and Problem Solved

In any API-driven application, a reliable method for validating the formats of incoming requests and outgoing responses is necessary. Without such a mechanism, the application is vulnerable to incorrect data, which leads to runtime errors, complicates debugging, and makes the API contract implicit and fragile.

`models.py` solves this problem by using Pydantic to create a Single Source of Truth for all data models. This allows for:
*   Automatic validation of all incoming requests, rejecting incorrect ones at the system's "entry point."
*   Ensuring that the responses generated by the application always conform to a predefined structure.
*   Providing a clear, machine-readable contract that the FastAPI framework uses to automatically generate interactive documentation.

## 3. Key Models and Architectural Decisions

### 3.1. OpenAI Standard Compliance (Core Principle)

The vast majority of models in this file (`ChatCompletionRequest`, `EmbeddingResponse`, etc.) are intentionally and precisely designed to duplicate the structure of the official OpenAI API.

**Value:** This approach turns the gateway into a "drop-in" replacement for the standard OpenAI client. Any existing application can be redirected to this service simply by changing the base URL, without needing to rewrite the code that interacts with the API.

### 3.2. Support for Various Modalities

The models are grouped by the types of tasks they serve:

*   **Chat Completions (Text Generation):**
    *   `ChatCompletionRequest`: Defines the structure of an incoming request, including messages, generation parameters, and tools.
    *   `ChatCompletionResponse`: The standard response for synchronous requests.
    *   `ChatCompletionChunk`: A specialized model for streaming, describing a single "chunk" of the response.
*   **Embeddings (Vector Representations):**
    *   `EmbeddingRequest`: A model for requesting embeddings for a string or a list of strings.
    *   `EmbeddingResponse`: A standardized response containing the vector representations.
*   **Audio (Speech and Transcription):**
    *   `TranscriptionModel` / `TranscriptionResponse`: Models for speech-to-text (STT) tasks.
    *   `SpeechCreationRequest`: A model for text-to-speech (TTS) tasks. This model has been updated to fully comply with the latest OpenAI TTS API versions, including fields like `voice`, `response_format`, and `speed`.

### 3.3. Proprietary Model for the AI Agent (ReactRequest)

Unlike the others, the `ReactRequest` model does not conform to any external standard. It defines a high-level, internal API contract for running the application's unique logicâ€”the ReAct agent.

**Architectural Nuance:** This model is the entry point for the service's most complex and unique functionality. It aggregates parameters for managing sessions (`session_id`), agent behavior (`reasoning_mode`), and allows the client to pass specific instructions and "manifests" (`client_system_instruction`, `client_manifests`).

## 4. API Usage Examples

This section demonstrates how the Pydantic models from this module correspond to real HTTP requests to the gateway's API.

### 4.1. Chat Completion Request (Standard)

This example shows a standard, non-streaming request to a chat model. It uses the `ChatCompletionRequest` model.

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
  "model": "gpt-4-turbo",
  "messages": [
    {
      "role": "user",
      "content": "What is the capital of France?"
    }
  ],
  "max_tokens": 50,
  "stream": false
}'
```
*   `model`: Specifies the alias (`model_alias`) from `proxy_config.yaml`.
*   `stream: false`: Ensures the server will return a full JSON response (`ChatCompletionResponse`) after generation is complete.

### 4.2. Chat Completion Request (Streaming)

For interactive applications, the same endpoint is used, but with the `stream: true` flag.

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
  "model": "gpt-4-turbo",
  "messages": [
    {
      "role": "user",
      "content": "Write a short story about a robot who discovers music."
    }
  ],
  "stream": true
}'
```
*   `stream: true`: Instructs the server to send the response in parts in the Server-Sent Events (SSE) format. Each part will correspond to the `ChatCompletionChunk` model.

### 4.3. Embeddings Request

This example shows how to get vector representations (embeddings) for text using the `EmbeddingRequest` model.

```bash
curl -X POST http://localhost:8000/v1/embeddings \
-H "Content-Type: application/json" \
-d '{
  "model": "text-embedding-model",
  "input": "Universal AI Gateway provides reliable and scalable access to AI models."
}'
```
The server will return a response corresponding to the `EmbeddingResponse` model, with a list of vectors.

### 4.4. Text-to-Speech (TTS) Request

This example demonstrates generating speech from text using the `SpeechCreationRequest` model.

```bash
curl -X POST http://localhost:8000/v1/audio/speech \
-H "Content-Type: application/json" \
-d '{
  "model": "tts-model-alias",
  "input": "Hello, world! This is the universal AI gateway.",
  "voice": "alloy",
  "speed": 1.1
}' \
--output speech.mp3
```
*   `--output speech.mp3`: A `curl` flag to save the streaming audio response to a file.
*   `voice`, `speed`: Optional parameters that the proxy will pass to the target TTS provider.

### 4.5. Request to Start a ReAct Session (Enrichment)

This is the most powerful endpoint, using the proprietary `ReactRequest` model to run the reasoning and enrichment engine.

```bash
curl -X POST http://localhost:8000/v1/react/sessions \
-H "Content-Type: application/json" \
-d '{
  "user_query": "What is the capital of France and what is the weather there right now?",
  "model_alias": "smart_agent",
  "session_id": "user123-abc",
  "client_system_instruction": "Respond as a helpful and witty tour guide."
}'
```
*   `user_query`: A query requiring complex logic (in this case, two steps: find the capital, then find the weather).
*   `model_alias`: Must point to a model for which `reasoning_mode` is configured in `proxy_config.yaml`.
*   `session_id`: (Optional) Allows tracking or resuming sessions.

The server will respond with a stream of Server-Sent Events (SSE), broadcasting all stages of the agent's work (thoughts, tool calls, final answer).

## 5. Architectural Value and Business Impact

*   **Reliability and Stability:** Strict validation based on Pydantic models prevents an entire class of errors related to incorrect data.
*   **Accelerated Development:** Automatic documentation generation by FastAPI and static typing significantly improve the Developer Experience.
*   **Strategic Compatibility:** Adhering to the OpenAI standard is a key business decision, making the product easily integrable into the existing ecosystem of AI tools.
*   **Clear Contract Definition:** The module serves as an explicit and unambiguous "contract" about the form in which data should enter and leave the system.